# ---- DATASET: OC20 IS2RE-all split ----
#
# *** Important note ***
#   The total number of gpus used for this run was 1.
#   If the global batch size (num_gpus * batch_size) is modified
#   the lr_milestones and warmup_steps need to be adjusted accordingly.
default:
  frame_averaging: 2D # {"", 2D, 3D, DA}
  fa_method: se3-stochastic # {"", all, stochastic, det, se3-all, se3-stochastic, se3-det}
  model:
    name: faenet
    act: swish # activation function
    hidden_channels: 384 
    num_filters: 480 # size of convolution filter
    num_interactions: 5 # number of interaction blocks
    num_gaussians: 104 # size of radial gaussian basis
    cutoff: 6.0 # cutoff distance
    preprocess: pbc_preprocess # {pbc_preprocess, base_preprocess, null}
    regress_forces: False # {False, from_energy, direct, direct_with_gradient_target}
    tag_hidden_channels: 64  # only for OC20
    pg_hidden_channels: 64  # period & group embedding hidden channels
    phys_embeds: True  # physics-aware embeddings for atoms
    phys_hidden_channels: 0
    num_chemical_elements: 85 # total number of unique atom types in dataset
    energy_head: weighted-av-final-embeds  # Energy head: {False, weighted-av-initial-embeds, weighted-av-final-embeds}
    skip_co: concat  # Skip connections {False, "add", "concat"}
    second_layer_MLP: False  # in EmbeddingBlock
    complex_mp: True  # 2-layer MLP in Interaction blocks
    mp_type: base  # Message Passing type {'base', 'simple', 'updownscale', 'updownscale_base'}
    graph_norm: True  # graph normalization layer
    force_decoder_type: "mlp" # force head (`"simple"`, `"mlp"`, `"res"`, `"res_updown"`)
    force_decoder_model_config:
      simple:
        hidden_channels: 128
        norm: batch1d # batch1d, layer or null
      mlp:
        hidden_channels: 256
        norm: batch1d # batch1d, layer or null
      res:
        hidden_channels: 128
        norm: batch1d # batch1d, layer or null
      res_updown:
        hidden_channels: 128
        norm: batch1d # batch1d, layer or null
  optim:
    batch_size: 256
    eval_batch_size: 256
    max_epochs: 12
    scheduler: LinearWarmupCosineAnnealingLR
    optimizer: AdamW
    warmup_steps: 6000
    warmup_factor: 0.2
    lr_initial: 0.002
    lr_gamma: 0.1
    lr_milestones:
      - 18000
      - 27000
      - 37000